# WebcrawlerforCOVID-19
A crawler for getting data about COVID-19 on Sina Weibo.

该爬虫程序主要分为以下几个部分：

1、	网址解析 (Spidermainbykeyword.py)

 a)	这一步主要分析需要爬取的目标网址，该爬虫目的是通过关键词、时间、地点等条件筛选，找到符合条件的微博。因为新浪微博有返回数量限制，每次检索返回最多1000条微博，因此，检索条件要尽量详细，可以避免数据不完整。

 b)	解析url，如果获取关键词为“干咳”、地点对应“福建”、时间对应“2019-11-01-00：2019-11-01-01”内的所有微博，就设定如下url： (Keywords.py)
https://s.weibo.com/weibo?q=干咳&region=custom:35&timescope=custom:2019-11-01-00:2019-11-01-06

 c)	根据规则，把所有想要爬取的网址都存入数据库中

2、	网页分析 (Htmlparser.py)

 a)	从数据库中获取一个网址，并标记已爬取

 b)	解析该网页的其他可爬取url
 
 c)	解析该网页可爬取的内容
 
 d)	应对微博反爬机制

  i.	模拟浏览器登录

  ii.	模拟微博账号 (CookieManager.py)

  获取微博内容需要登录账号，否则返回的不是想要获得的信息。可以使用（多个）微博账号，在登录的时候，从浏览器（谷歌、火狐）中获取cookie，在爬虫时加入cookie。Cookie容易失效，因此需要隔一段时间更新。

  iii.	设置爬取时间间隔
访问数据的频率不宜太高，容易被检测异常，封号封IP，因此需要设置时间间隔，并且尽量使随机数。

  iv.	建立IP池，防止电脑IP被封 (IpManager.py)
如果一直使用自己的电脑IP爬取数据，容易被微博会封掉IP。解决这个问题可以购买Ip，也可以自己从网上找免费IP，但是容易失效。

3、	数据存储 (SqlManager.py)

 a)	url存到数据库中

 b)	数据字段存到数据库中
